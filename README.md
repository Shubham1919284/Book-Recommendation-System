# ğŸ“š Book Recommendation System

**Item-Based Collaborative Filtering using KNN**

---

## ğŸ“Œ Overview

This project implements an **item-based collaborative filtering recommendation system** that suggests books based on **similar user rating patterns**.
Instead of using book content, the system identifies relationships between books by analyzing how users rate them.

---

## ğŸ¯ Objective

* Build a reliable recommendation system using user behavior
* Reduce data sparsity for better similarity calculation
* Generate meaningful book recommendations
* Keep the model simple, interpretable, and scalable

---

## ğŸ§  Approach

* **Recommendation Type:** Item-Based Collaborative Filtering
* **Algorithm:** K-Nearest Neighbors (KNN)
* **Similarity Metric:** Cosine Similarity

**Why Item-Based?**
Item relationships are more stable than user preferences and scale better for large datasets.

---

## ğŸ“‚ Dataset

**Goodbooks-10K Dataset**

* `books.csv` â€“ Book metadata
* `ratings.csv` â€“ User-book ratings

This dataset reflects real-world sparsity, making it suitable for collaborative filtering.

---

## ğŸ”§ Tech Stack

* **Language:** Python
* **Libraries:** Pandas, NumPy, Scikit-learn
* **Visualization:** Matplotlib, Seaborn

---

## âš™ï¸ Data Preprocessing

To improve recommendation quality, interaction sparsity is reduced by filtering:

* Books with **at least 50 ratings**
* Users who have rated **at least 50 books**

This ensures reliable similarity computation and removes noisy data.

---

## ğŸ§± User-Item Matrix

Ratings are converted into a matrix format:

* Rows â†’ Books
* Columns â†’ Users
* Values â†’ Ratings

Missing values are filled with `0`, representing no interaction.

---

## ğŸ¤– Model Training

* **Model:** Nearest Neighbors
* **Distance Metric:** Cosine
* **Algorithm:** Brute Force

Cosine similarity is used because it performs well with sparse, high-dimensional data.

---

## ğŸ” Recommendation Logic

Given a book title:

1. Locate the book in the matrix
2. Find nearest books based on cosine similarity
3. Exclude the input book itself
4. Return the most similar books

---

## ğŸ“ˆ Strengths

* Simple and interpretable approach
* Handles sparse data effectively
* No complex feature engineering
* Easy to explain in interviews

---

## âš ï¸ Limitations

* Cold-start problem for new users/books
* No content-based recommendations
* Depends entirely on user ratings

---

## ğŸš€ Future Enhancements

* Hybrid recommender system
* Matrix factorization (SVD)
* Streamlit-based web app
* API deployment

---

## ğŸ“¬ Author

**Shubham Kumar Jha**
ğŸ”— GitHub: [https://github.com/Shubham1919284](https://github.com/Shubham1919284)
ğŸ”— LinkedIn: [https://www.linkedin.com/in/shubham-kumar-jha-1a2b3c](https://www.linkedin.com/in/shubham-kumar-jha-1a2b3c)

